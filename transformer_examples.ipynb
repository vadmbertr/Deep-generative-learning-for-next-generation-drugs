{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadmbertr/Deep-generative-learning-for-next-generation-drugs/blob/main/transformer_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkpgg1-RnOBz"
      },
      "source": [
        "Charger un modèle de langage pré-entraîné de type transformer (avec son tokéniseur) pour le français à partir du dépôt de HuggingFace :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRSi8oannOB1",
        "outputId": "f0d5c02e-aa1f-40e8-cf1d-8fc1ee15e57a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "from pprint import pprint\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "HF_MODEL_CACHE_DIR = \"c:/Users/ait/.cache/huggingface/transformers\"\n",
        "os.environ['TRANSFORMERS_CACHE'] = HF_MODEL_CACHE_DIR\n",
        "\n",
        "\n",
        "# French models: ClassCat/roberta-base-french, Yanzhu/bertweetfr-base, Geotrend/distilbert-base-en-fr-es-pt-it-cased, \n",
        "pretrained_model_name = \"camembert-base\" \n",
        "# pretrained_model_name = \"ClassCat/roberta-base-french\"\n",
        "\n",
        "lmtokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "lm = AutoModel.from_pretrained(pretrained_model_name, output_attentions=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_JVqDKPnOB3"
      },
      "source": [
        "Tokéniser le premier texte avec le tokéniseur du modèle de langage :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-5FcAvjnOB4",
        "outputId": "be313f0d-cb19-4326-e1e4-46771cca9248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '▁Film', '▁gentil', '▁et', '▁très', '▁dis', 'tr', 'ayant', '.', '</s>']\n",
            "['<s>', '▁J', \"'\", 'aime', '▁le', '▁ca', 'member', 't', '.', '</s>']\n",
            "['<s>', '▁il', '▁lit', '▁un', '▁livre', '.', '</s>']\n",
            "['<s>', '▁il', '▁lit', '▁le', '▁journal', '.', '</s>']\n",
            "['<s>', '▁Des', '▁draps', '▁et', '▁un', '▁lit', '▁pour', '▁dormir', '.', '</s>']\n",
            "['<s>', '▁Il', '▁s', \"'\", 'allonge', 'a', '▁sur', '▁le', '▁lit', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "    \"Film gentil et très distrayant. \",\n",
        "    \"J'aime le camembert.\",\n",
        "    \"il lit un livre.\",\n",
        "    \"il lit le journal.\",\n",
        "    \"Des draps et un lit pour dormir.\",\n",
        "    \"Il s'allongea sur le lit.\",\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    tokens = lmtokenizer.tokenize(text, add_special_tokens=True)\n",
        "    print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbkO9C8-nOB5"
      },
      "source": [
        "Tokéniser et encoder les textes pour servir d'entrée au modèle de langage pré-entrainé : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axBTOwSxnOB5",
        "outputId": "81682db8-d569-4633-e5a0-8613bf87a2db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
            " 'input_ids': tensor([[    5,  5897,  4845,    14,    95,   701,  4871,  3276,     9,     6,\n",
            "             1],\n",
            "        [    5,   121,    11,   660,    16,   730, 25543,   110,     9,     6,\n",
            "             1],\n",
            "        [    5,    51,   849,    23,   510,     9,     6,     1,     1,     1,\n",
            "             1],\n",
            "        [    5,    51,   849,    16,  2353,     9,     6,     1,     1,     1,\n",
            "             1],\n",
            "        [    5,   363, 16507,    14,    23,   849,    24,  4655,     9,     6,\n",
            "             1],\n",
            "        [    5,    69,    52,    11, 12435,    55,    32,    16,   849,     9,\n",
            "             6]])}\n"
          ]
        }
      ],
      "source": [
        "encoded_texts = lmtokenizer(texts, \n",
        "    padding=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='pt',\n",
        "    return_offsets_mapping=False,\n",
        ")\n",
        "\n",
        "pprint(encoded_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn2cPh11nOB5"
      },
      "source": [
        "Appliquer le modèle sur les textes et afficher le vecteur de représentation du 2ème mot (\"gentil\") du premier texte. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8hSnmWDnOB6",
        "outputId": "7251010b-3f03-4a51-cfb5-7dea7aa0d458"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 2.7026e-02, -2.2273e-01,  3.4151e-02, -1.2499e-01, -2.5172e-02,\n",
              "        -5.6220e-02, -1.1643e-05,  1.9083e-02,  5.9024e-02,  5.3655e-02,\n",
              "         1.2652e-03, -1.4679e-01,  6.7243e-02,  8.0300e-02,  8.1018e-02,\n",
              "        -2.0575e-03, -2.9184e-02, -7.4812e-02,  2.9311e-03, -5.7235e-02,\n",
              "        -1.7323e-01,  7.9357e-02,  9.2863e-02, -4.5485e-02, -3.1706e-01,\n",
              "        -2.3649e-02, -2.6752e-01, -2.8997e-02,  5.0029e-02,  4.0398e-02,\n",
              "         9.2951e-02,  1.8293e-01,  1.6644e-01, -3.3054e-02,  4.2189e-02,\n",
              "         5.8270e-02, -3.6130e-02,  1.9843e-01, -2.1253e-02,  3.2097e-01,\n",
              "        -2.0587e-01,  6.0121e-02,  4.7284e-02, -8.5656e-02,  1.2964e-02,\n",
              "        -1.1330e-03,  1.5728e-02, -1.6476e-01,  3.8387e-02,  4.3340e-02,\n",
              "         3.1881e-02, -3.2446e-02, -9.2199e-02,  5.0469e-02, -1.2656e-01,\n",
              "        -7.8073e-02,  4.5900e-02, -1.8651e-01, -9.5165e-03, -8.5449e-02,\n",
              "         1.0050e-01, -8.5659e-02,  3.7343e-02, -1.7142e-02, -1.3409e-01,\n",
              "         1.0619e-02, -2.1054e-01, -3.0255e-02, -3.6309e-02, -2.3304e-01,\n",
              "        -3.8640e-02,  9.7420e-02, -1.2365e-01,  1.2345e-01, -6.6723e-02,\n",
              "        -8.8623e-03, -1.8112e-01, -9.3732e-02, -1.2966e-01,  4.7385e-02,\n",
              "        -9.8122e-02, -3.9433e-01,  6.5911e-02,  1.7415e-02,  5.1502e-03,\n",
              "         3.2484e-01,  2.0261e-01, -2.1219e-01, -7.2551e-03, -1.0044e-01,\n",
              "         7.4922e-01,  4.0126e-02,  1.0923e-01,  4.5062e-02, -3.3960e-01,\n",
              "         2.6800e-01, -1.7493e-01, -1.0516e-01,  7.8954e-02,  1.5351e-01,\n",
              "         1.6087e-01, -1.4204e-01, -7.8929e-02,  2.2868e-01, -5.9914e-03,\n",
              "         6.0164e-02, -1.5132e-01, -7.9620e-02, -7.1917e-02,  2.1177e-02,\n",
              "        -6.0316e-02, -1.1362e-01, -7.3065e-03, -7.9485e-02,  4.2587e-02,\n",
              "         5.0327e-01,  7.7221e-02,  1.4113e-02, -9.5454e-02,  2.7376e-02,\n",
              "        -2.1791e-01,  1.4876e-02, -1.2436e-01,  3.3303e-02,  1.1316e-01,\n",
              "         3.0108e-02, -9.2944e-02,  2.0133e-02,  6.7099e-02, -6.7213e-02,\n",
              "        -1.3819e-01,  1.5296e-02,  1.0438e-01,  1.1558e-01,  1.6328e-01,\n",
              "         1.6703e-02,  1.1536e-01, -9.5929e-02,  8.7805e-02,  2.3199e-02,\n",
              "         2.2183e-01, -8.7924e-02,  8.6416e-02,  9.4383e-02, -1.6441e-01,\n",
              "         7.0834e-02,  4.6680e-02,  3.1582e-01,  4.1669e-03, -2.7902e-04,\n",
              "         7.8206e-02,  1.1771e-01, -1.3959e-01,  4.8821e-02, -1.8798e-02,\n",
              "        -9.3766e-02, -6.1993e-02, -3.2765e-02,  6.3963e-02,  1.0453e-01,\n",
              "        -1.9629e-01, -2.7464e-01,  1.8485e-02,  2.6332e-01,  6.3817e-02,\n",
              "         6.7413e-02,  6.5876e-02,  7.5857e-02,  1.7996e-02,  1.4778e-01,\n",
              "         7.7375e-02,  6.0842e-02, -1.5682e-01,  8.7592e-02, -1.0605e-01,\n",
              "        -1.4339e-02, -5.9669e-02,  7.4346e-02,  1.3456e-01,  7.1424e-02,\n",
              "         1.8993e-02, -1.1130e-01,  9.8312e-03, -1.0294e-01,  3.4828e-02,\n",
              "         1.2839e-01, -1.6519e-02,  3.3704e-01,  2.1976e-01,  4.1346e-02,\n",
              "         1.8750e-01, -3.4196e-02,  2.5504e-02, -2.3940e-01, -1.4079e-01,\n",
              "         9.2293e-02, -1.5315e-01, -3.2455e-02,  3.7046e-02, -4.5955e-02,\n",
              "         7.7210e-02, -2.0391e-02, -3.3210e-02,  9.5035e-02, -9.3085e-02,\n",
              "        -6.5339e-02, -8.7547e-02, -6.6876e-02, -1.4506e-01, -9.8224e-02,\n",
              "         4.1708e-01, -1.8463e-02, -5.9985e-02, -8.8515e-02,  9.0817e-02,\n",
              "        -1.6546e-01,  9.8233e-02, -5.0222e-02,  1.1034e-01,  1.0718e-01,\n",
              "         2.1066e-01, -9.7860e-02,  1.5834e-01, -4.4732e-02, -2.7328e-01,\n",
              "        -1.0384e-01,  1.5588e-03,  1.6263e-02, -9.3000e-02,  1.4279e-01,\n",
              "         7.6515e-02,  7.5870e-03, -9.4370e-02, -7.7794e-03, -4.2516e-02,\n",
              "         4.6055e-02,  6.0700e-02, -2.6493e-01,  7.0248e-02, -1.5098e-02,\n",
              "        -2.2248e-01,  5.7897e-02,  1.4105e-01,  1.6179e-01, -4.6513e-02,\n",
              "        -8.9457e-03, -8.2988e-02,  1.2982e-01,  1.1641e-01,  3.6042e-01,\n",
              "        -9.1061e-02, -9.4693e-02, -3.0462e-01, -1.2764e-02, -1.0096e-06,\n",
              "        -1.7683e-02, -1.7663e-01, -4.3466e-02,  4.1445e-01,  3.2344e-02,\n",
              "        -6.3779e-02,  1.8247e-01, -1.1605e-01, -2.9299e-02,  1.2855e-01,\n",
              "        -5.8676e-02,  7.3324e-02,  1.5943e-01,  2.2952e-01, -1.4221e-01,\n",
              "         5.5394e-01, -1.2873e-01, -8.9784e-02, -1.4828e-01,  4.6025e+00,\n",
              "         1.4697e-01, -6.9011e-03,  3.2826e-02,  4.5663e-01,  2.0917e-02,\n",
              "         6.6900e-02, -3.0548e-02,  1.5595e-02, -1.0176e-01,  1.6177e-01,\n",
              "         1.8888e-02,  2.1500e-01,  9.9046e-02,  5.7237e-02,  4.4686e-02,\n",
              "         4.5653e-03,  3.8959e-02,  5.8708e-02, -1.0256e-01,  8.4488e-03,\n",
              "         9.1091e-02, -8.7972e-02,  1.2975e-01,  4.4229e-02, -1.5323e-01,\n",
              "         8.8851e-02,  1.4550e-01, -1.3723e-01,  2.8323e-02,  4.7318e-02,\n",
              "         1.0429e-01, -8.2850e-02,  1.5370e-01, -2.3462e-02,  1.3881e+00,\n",
              "         9.8789e-02, -5.4155e-02, -4.7038e-02,  7.3631e-02,  6.5009e-02,\n",
              "         9.1833e-02,  6.7342e-02,  1.1558e-01,  3.5258e-01, -1.4014e-02,\n",
              "         1.8739e-01, -8.6645e-02,  3.2613e-02,  5.7285e-03,  4.6530e-02,\n",
              "        -4.6289e-02, -6.4868e-02,  1.4964e-01, -3.0113e-02, -2.2610e-01,\n",
              "         4.7723e-02,  6.5102e-01,  8.6877e-02, -5.8702e-02,  1.0233e-01,\n",
              "         4.7140e-02, -4.2640e-02,  1.0614e-01, -2.8111e-02, -4.6093e-02,\n",
              "        -9.7204e-02,  2.3016e-02,  1.7328e-01,  1.1851e-01, -1.0846e-01,\n",
              "        -1.0784e-02,  9.9423e-03, -4.2580e-02, -3.5492e-01, -9.7421e-02,\n",
              "        -3.2364e-02,  5.7826e-02, -1.5542e-01, -1.9350e-02, -7.2801e-02,\n",
              "         2.8496e-02, -1.6563e-02,  2.9154e-01,  3.4988e-02,  2.5820e-01,\n",
              "        -5.6936e-02,  3.2404e-02,  3.0605e-02, -1.2079e-02, -1.6479e-01,\n",
              "         8.4571e-03, -1.0574e-01,  1.6592e-01, -2.7954e-01,  2.8572e-02,\n",
              "        -4.1326e-02, -8.4872e-02, -1.5295e-01, -2.4929e-01, -5.7408e-02,\n",
              "         1.0671e-01,  1.0470e-01, -1.0242e-01,  1.6111e-02, -6.5812e-02,\n",
              "        -2.8123e-01,  1.5107e-01,  4.4391e-02,  1.2283e-01,  4.6833e-02,\n",
              "        -7.0286e-02, -9.2259e-02, -5.2424e-02, -3.1014e-01, -1.9939e-01,\n",
              "        -3.9325e-02, -2.2159e-01, -5.5124e-02,  9.1702e-02, -3.5328e-02,\n",
              "        -1.0727e-01,  7.3727e-02, -1.0089e-02, -5.7155e-02, -9.0273e-02,\n",
              "         2.4252e-01,  3.3670e-02, -5.2564e-02,  2.4107e-01,  1.4615e-01,\n",
              "         6.5281e-02,  1.3823e-02,  9.6027e-02, -1.4768e-01,  2.4041e-02,\n",
              "        -2.1232e-02, -4.9784e-02,  2.7754e-01,  5.3353e-02,  5.2294e-02,\n",
              "         6.3451e-03, -2.7282e-02,  2.6819e-01,  4.4365e-01,  1.6147e-02,\n",
              "        -2.4705e-03, -1.1732e-01, -1.2309e-02,  1.6432e-01,  8.0145e-02,\n",
              "         7.4825e-02, -1.1821e-01, -7.6164e-02,  6.7569e-02,  7.2042e-02,\n",
              "         3.3112e-02,  7.6173e-02,  1.3604e-01, -1.7494e-01,  1.5419e-01,\n",
              "         1.5623e-01, -2.4404e-01, -2.4958e-02,  9.1710e-02, -1.0643e-01,\n",
              "        -3.0109e-01, -1.6981e-01,  1.6236e-01,  9.7054e-02,  6.7295e-02,\n",
              "        -3.4302e-02,  1.0468e-01,  1.9094e-01, -1.2867e-02,  2.8811e-02,\n",
              "        -7.5309e-02,  1.5620e-01, -7.1295e-03, -1.1041e-01, -2.5839e-02,\n",
              "        -2.8531e-02,  1.1241e-01,  2.9154e-01,  8.1420e-02,  1.5667e-01,\n",
              "        -1.4834e-01, -1.2930e-01, -6.6687e-02,  5.0678e-02, -6.4436e-02,\n",
              "         1.5672e-01,  9.4318e-02,  1.0625e-01,  5.5648e-02,  9.5213e-02,\n",
              "         3.7243e-02, -8.7646e-03,  9.5903e-02, -6.1950e-01, -4.5974e-02,\n",
              "        -1.4933e-01, -4.6831e-02, -7.3211e-02,  4.5657e-02,  3.8240e-02,\n",
              "         5.7597e-02,  7.0005e-02, -3.9715e-02,  1.6546e-01,  5.6502e-02,\n",
              "        -2.4118e-02,  1.1329e-01, -1.0800e-01, -4.4386e-02,  9.9546e-03,\n",
              "        -2.2946e-02, -8.9545e-02,  2.2403e-01,  8.4219e-02,  9.4221e-02,\n",
              "         8.8008e-02,  1.6177e-01,  1.1347e-01, -3.6214e-02,  9.9742e-02,\n",
              "        -1.8733e-01,  5.3304e-02, -2.3258e-01,  1.4685e-01, -1.0235e-01,\n",
              "        -4.6193e-02, -1.6069e-02,  8.0398e-02,  1.7466e-02,  2.8075e-01,\n",
              "         8.1067e-02,  5.9324e-02, -1.7291e-01,  1.3078e-01, -7.0852e-02,\n",
              "        -7.3126e-02, -9.3628e-02, -2.6738e-02, -6.5086e-03, -1.0694e-01,\n",
              "        -5.1942e-02, -1.5285e-01, -1.5688e-01, -3.9854e-02,  6.4059e-02,\n",
              "         2.3993e-01,  1.9643e-01,  1.0492e-01,  1.2993e-01, -9.3027e-02,\n",
              "        -4.5653e-02,  1.3703e-01,  1.5264e-01,  7.8359e-02,  7.1255e-02,\n",
              "         3.9747e-02,  1.1425e-02,  1.5971e-01,  5.4224e-02,  1.2721e-01,\n",
              "        -7.6441e-03, -6.5076e-02,  2.0367e-01,  3.0860e-01,  1.0180e-01,\n",
              "         1.2716e-01, -1.5511e-01,  6.4231e-02,  1.1870e-01,  1.8665e-01,\n",
              "        -3.1310e-01,  5.7238e-02,  3.7917e-02,  2.0186e-02,  8.5771e-02,\n",
              "         7.9076e-03,  2.4215e-02, -1.0857e-01, -1.9071e-01,  3.6724e-02,\n",
              "         3.7981e-02,  7.4035e-02,  2.9179e-03, -8.7637e-02, -1.7349e-01,\n",
              "         1.4495e-01, -5.0574e-02, -1.3476e-01,  9.5952e-02, -6.6353e-02,\n",
              "        -3.7599e-02,  8.3413e-02, -2.6543e-01, -1.8094e-02,  4.8416e-02,\n",
              "        -6.0553e-01, -1.0644e-01,  3.5972e-02, -1.7248e-01, -4.3777e-02,\n",
              "        -2.4133e-01, -1.2791e-02, -1.0801e-01, -3.1875e-01,  9.4741e-02,\n",
              "         1.9457e-01,  7.2904e-02, -1.9242e-02,  1.7170e-01, -2.6431e-02,\n",
              "        -5.1292e-02,  2.0228e-01, -1.6971e-01,  2.9635e-02, -4.8398e-02,\n",
              "        -1.0925e-02, -6.7398e-02,  7.0221e-02, -1.1106e-01, -1.9268e-01,\n",
              "        -2.4000e-01, -2.3906e-02,  8.5450e-02,  3.0556e-03, -5.4770e-02,\n",
              "         1.2355e-01,  6.8659e-02,  4.2170e-01,  7.2791e-02, -5.6583e-02,\n",
              "         2.3937e-03,  1.1606e-01, -2.1042e-01,  2.3183e-01, -8.9431e-02,\n",
              "         9.5373e-02,  5.0446e-02, -8.7180e-02,  1.3833e-01, -2.0774e-01,\n",
              "        -6.5357e-02,  1.1889e-01, -5.1143e-02,  3.9418e-01, -1.2712e-01,\n",
              "        -3.5355e-02,  2.0614e-01,  1.8023e-01, -1.6080e-01, -1.7840e-01,\n",
              "         3.9394e-02,  3.0007e-01, -1.8681e-01,  5.6143e-01, -2.2347e-01,\n",
              "        -3.4365e-02, -1.2542e-01,  2.6494e-02,  1.3328e-01, -1.3602e-01,\n",
              "         2.3607e-02,  1.4949e-01, -2.1554e-03,  1.0895e-01,  9.6912e-02,\n",
              "         2.4862e-01,  9.5716e-02,  1.4321e-02,  4.7583e-02,  4.1898e-02,\n",
              "        -4.1257e-02,  6.0104e-02,  1.0800e-01, -5.7315e-02,  1.1190e-01,\n",
              "         2.1327e-01, -5.8993e-02,  3.5852e-02,  6.2003e-02, -9.4064e-02,\n",
              "        -2.2071e-02, -5.7708e-02,  2.8687e-01, -3.7980e-02, -1.0866e-01,\n",
              "        -6.1319e-02,  1.2815e-01, -3.7379e-02,  1.5382e-02,  5.5006e-02,\n",
              "        -2.7977e-02, -2.2500e-02,  6.3256e-02,  1.7855e-01, -1.9771e-02,\n",
              "         2.8857e-02,  1.3964e-02,  2.5820e-02, -9.7809e-03, -3.2239e-02,\n",
              "         8.5673e-03,  1.7711e-02,  5.4366e-02, -1.1533e-01, -7.5830e-02,\n",
              "         1.0858e-01,  1.4889e-01,  2.0997e-02, -1.2747e+00,  3.5322e-02,\n",
              "        -2.2108e-01,  2.6540e-02, -1.1786e-01, -3.0409e-01,  1.1310e-01,\n",
              "         6.5211e-03, -3.9568e-02, -1.0826e-01,  2.4189e-02,  9.6484e-02,\n",
              "        -3.0423e-02, -1.6320e-01, -4.9217e-02,  6.5173e-02,  1.3378e-01,\n",
              "        -9.9650e-02,  4.0890e-02, -4.2492e-02,  1.4357e-02,  1.5342e-01,\n",
              "        -3.5473e-02, -2.4118e-01,  3.5906e-02, -9.6820e-02, -2.5238e-02,\n",
              "        -1.5120e-01, -8.8178e-02, -2.9574e-02,  8.3812e-02,  2.2740e-01,\n",
              "        -1.5597e-02,  9.9165e-02, -2.0304e-01,  5.4710e-02,  1.0284e-03,\n",
              "         1.2367e-01, -3.4901e-02, -1.3697e-01, -7.8792e-02, -8.3407e-02,\n",
              "        -1.1172e-01, -6.5989e-02,  1.1024e-01,  8.3297e-02,  4.0373e-02,\n",
              "        -1.6358e-02, -2.6507e-02, -3.3227e-02,  4.3561e-03,  1.9678e-02,\n",
              "         9.1464e-02, -4.6720e-02,  3.1566e-01, -1.6229e-01, -1.0031e-02,\n",
              "         7.2883e-02, -1.3563e-01, -3.8564e-02,  1.4445e-01, -3.1495e-01,\n",
              "         1.4323e-01,  2.6922e-02, -1.3019e-01, -3.8691e-02,  2.1855e-01,\n",
              "        -1.4920e-01, -8.1270e-02,  2.8017e-01,  6.6620e-02, -3.3118e-02,\n",
              "         1.5193e-01,  2.8241e-01,  1.6835e-02, -4.2246e-02,  5.9196e-02,\n",
              "         9.6289e-02,  5.7490e-02, -1.1279e-02], grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm.eval()\n",
        "\n",
        "# obtenir les vecteurs de représentation contextuels des textes\n",
        "output = lm.forward(input_ids=encoded_texts['input_ids'], attention_mask=encoded_texts['attention_mask'])[0]\n",
        "\n",
        "# afficher le vecteur de représentation du 2ème mot du premier texte\n",
        "output[0][2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnV2-WAlnOB7"
      },
      "source": [
        "Quelques exemples de scores de similarité entre des mots dans les textes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1EhL1bInOB8",
        "outputId": "ae270997-c72a-4493-e75d-4489714404f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9569, grad_fn=<SumBackward1>)\n",
            "tensor(0.4915, grad_fn=<SumBackward1>)\n",
            "tensor(0.7944, grad_fn=<SumBackward1>)\n",
            "tensor(0.5691, grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# similarity between word \"lit\" in text 3 and word \"lit\" in text 4\n",
        "print(F.cosine_similarity(output[2][2], output[3][2], dim=0))\n",
        "# similarity between word \"lit\" in text 3 and word \"lit\" in text 5\n",
        "print(F.cosine_similarity(output[2][2], output[4][5], dim=0))\n",
        "# etc.\n",
        "print(F.cosine_similarity(output[4][5], output[5][8], dim=0))\n",
        "print(F.cosine_similarity(output[2][2], output[5][8], dim=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9rWLdkgnOB9"
      },
      "source": [
        "Exemple de scores de similarité entre les textes entiers, en prenant comme représentant le vecteur du token de début `<s>`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQhhHj6rnOB9",
        "outputId": "4e9d6786-4bb8-49b0-a771-f9238b0c4d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9839, grad_fn=<SumBackward1>)\n",
            "tensor(0.9541, grad_fn=<SumBackward1>)\n",
            "tensor(0.9527, grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# similarity entre phrases\n",
        "print(F.cosine_similarity(output[2][0], output[3][0], dim=0))\n",
        "print(F.cosine_similarity(output[2][0], output[4][0], dim=0))\n",
        "print(F.cosine_similarity(output[4][0], output[5][0], dim=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzlJNr1cnOB9"
      },
      "source": [
        "Exemple de scores de similarité entre les textes entiers, en prenant comme représentant la moyenne des vecteurs de tous les tokens d'un texte :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbsa9xXRnOB-",
        "outputId": "b8f4fa2e-ccb8-471f-f884-392d4c780122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Film gentil et très distrayant. ', \"J'aime le camembert.\", 'il lit un livre.', 'il lit le journal.', 'Des draps et un lit pour dormir.', \"Il s'allongea sur le lit.\"]\n",
            "tensor(0.9443, grad_fn=<SumBackward1>)\n",
            "tensor(0.7651, grad_fn=<SumBackward1>)\n",
            "tensor(0.7442, grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "avgvects = output.mean(dim=1)\n",
        "print(texts)\n",
        "# similarity entre phrases\n",
        "print(F.cosine_similarity(avgvects[2], avgvects[3], dim=0))\n",
        "print(F.cosine_similarity(avgvects[2], avgvects[4], dim=0))\n",
        "print(F.cosine_similarity(avgvects[4], avgvects[5], dim=0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('ft')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cae357f383feff2d2c6d00b340476713021698fc9ab1dbe6e3120d10e4215bca"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}